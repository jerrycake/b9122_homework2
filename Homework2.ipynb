{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jerry\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://press.un.org/en/2023/sgsm21967.doc.htm\n",
      "https://press.un.org/en/2023/sgsm21947.doc.htm\n",
      "https://press.un.org/en/2023/dsgsm1874.doc.htm\n",
      "https://press.un.org/en/2023/sgsm21952.doc.htm\n",
      "https://press.un.org/en/2023/sgsm21876.doc.htm\n",
      "https://press.un.org/en/2023/sgsm21852.doc.htm\n",
      "https://press.un.org/en/2023/sgsm21806.doc.htm\n",
      "https://press.un.org/en/2023/dsgsm1848.doc.htm\n",
      "https://press.un.org/en/2023/sgsm21765.doc.htm\n",
      "https://press.un.org/en/2023/sgsm21767.doc.htm\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "BASE_URL = \"https://press.un.org\"\n",
    "SEED_URL = \"https://press.un.org/en\"\n",
    "PRESS_RELATIVE_URL = \"/en/press-release\"\n",
    "\n",
    "def is_press_release(soup):\n",
    "    \"\"\"Check if the page is a press release based on the 'PRESS RELEASE' link.\"\"\"\n",
    "    anchor = soup.find('a', hreflang='en', href=PRESS_RELATIVE_URL)\n",
    "    return anchor is not None\n",
    "\n",
    "def save_html_to_file(html, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        file.write(html)\n",
    "\n",
    "def get_press_releases_with_crisis(seed_url, part_number, limit=10):\n",
    "    visited = set()\n",
    "    to_visit = [seed_url]\n",
    "    press_releases = []\n",
    "\n",
    "    while to_visit and len(press_releases) < limit:\n",
    "        url = to_visit.pop(0)\n",
    "        if url in visited:\n",
    "            continue\n",
    "\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        if is_press_release(soup) and \"crisis\" in soup.get_text().lower():\n",
    "            press_releases.append(url)\n",
    "            save_html_to_file(response.text, f\"{part_number}_{len(press_releases)}.txt\")\n",
    "\n",
    "        visited.add(url)\n",
    "\n",
    "        # Extract links for further crawling\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            if link['href'].startswith('/'):\n",
    "                full_link = BASE_URL + link['href']\n",
    "                if full_link not in visited:\n",
    "                    to_visit.append(full_link)\n",
    "\n",
    "    return press_releases\n",
    "\n",
    "press_releases = get_press_releases_with_crisis(SEED_URL, 1, 10)\n",
    "for pr in press_releases:\n",
    "    print(pr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jerry\\AppData\\Local\\Temp\\ipykernel_12804\\639050535.py:22: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  tag1 = soup.find('span', class_=\"ep_name\", text='Plenary session')\n",
      "C:\\Users\\jerry\\AppData\\Local\\Temp\\ipykernel_12804\\639050535.py:23: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  tag2 = soup.find('span', class_=\"ep_name\", text='Press Releases')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "European Parliament press releases containing the word crisis\n",
      "https://www.europarl.europa.eu/news/en/press-room/20230310IPR77232/minimum-income-schemes-increasing-support-accessibility-and-inclusion\n",
      "https://www.europarl.europa.eu/news/en/press-room/20210422IPR02615/civil-protection-faster-eu-response-to-large-scale-emergencies\n",
      "https://www.europarl.europa.eu/news/en/press-room/20221209IPR64426/eu-long-term-budget-needs-urgent-revision-to-cope-with-current-crises\n",
      "https://www.europarl.europa.eu/news/en/press-room/20221209IPR64427/holodomor-parliament-recognises-soviet-starvation-of-ukrainians-as-genocide\n",
      "https://www.europarl.europa.eu/news/en/press-room/20230707IPR02421/parliament-adopts-new-rules-to-boost-energy-savings\n",
      "https://www.europarl.europa.eu/news/en/press-room/20210304IPR99207/parliament-gives-green-light-for-new-eu4health-programme\n",
      "https://www.europarl.europa.eu/news/en/press-room/20230210IPR74806/green-deal-industrial-plan-securing-the-eu-s-clean-tech-leadership\n",
      "https://www.europarl.europa.eu/news/en/press-room/20220321IPR25913/more-eu-action-needed-for-secure-food-supply\n",
      "https://www.europarl.europa.eu/news/en/press-room/20230929IPR06132/nagorno-karabakh-meps-demand-review-of-eu-relations-with-azerbaijan\n",
      "https://www.europarl.europa.eu/news/en/press-room/20220909IPR40138/parliament-adopts-new-rules-on-adequate-minimum-wages-for-all-workers-in-the-eu\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "\n",
    "seed_url = 'https://www.europarl.europa.eu/news/en/press-room'\n",
    "urls = [seed_url]\n",
    "seen = {seed_url}\n",
    "opened_press = set()\n",
    "min_links = 10\n",
    "\n",
    "def save_html_to_file(html, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        file.write(html)\n",
    "\n",
    "press_count = 0\n",
    "\n",
    "while len(urls) > 0 and press_count < min_links:\n",
    "    try:\n",
    "        curr_url = urls.pop(0)\n",
    "        req = urllib.request.Request(curr_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        webpage = urllib.request.urlopen(req).read()\n",
    "        soup = BeautifulSoup(webpage, 'html.parser')\n",
    "        tag1 = soup.find('span', class_=\"ep_name\", text='Plenary session')\n",
    "        tag2 = soup.find('span', class_=\"ep_name\", text='Press Releases')\n",
    "        if tag1 and tag2:\n",
    "            text = soup.get_text()\n",
    "            if 'crisis' in text.lower():\n",
    "                press_count += 1\n",
    "                opened_press.add(curr_url)\n",
    "                save_html_to_file(str(soup), f\"2_{press_count}.txt\")\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    for a_tag in soup.find_all('a', href=True):\n",
    "        org_child_url = a_tag.get('href')\n",
    "        child_url = urllib.parse.urljoin(seed_url, org_child_url)\n",
    "        if child_url not in seen and seed_url in child_url:\n",
    "            seen.add(child_url)\n",
    "            urls.append(child_url)\n",
    "\n",
    "print(\"European Parliament press releases containing the word crisis\")\n",
    "for link in opened_press:\n",
    "    print(link)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
